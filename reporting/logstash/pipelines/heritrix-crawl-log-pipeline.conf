#
# Accept logs entries from filebeat or via Kafka
#
input {
  beats {
    port => "5044"
    ssl => false
  }
#  kafka {
#    bootstrap_servers => "kafka:9092"
#    topics => ["heritrix"]
#    auto_offset_reset => "earliest"
#    codec => "json"
#  }
}

#
# Parse the Heritrix3 crawl log format
#
filter {
 if [type] == "heritrix" {

  # Parse overall log-line structure, including annotations and optional "extra fields" JSON chunk
  grok {
    match => [ "message", "%{NOTSPACE:log_timestamp} +%{NUMBER:fetch_status_code:int} +%{NOTSPACE:resource_size:int} %{NOTSPACE:downloaded_uri} %{NOTSPACE:discovery_path} %{NOTSPACE:referrer_uri} %{NOTSPACE:mime_type} %{NOTSPACE:worker_thread_id} %{NOTSPACE:fetch_timestamp} %{NOTSPACE:sha1_digest} %{NOTSPACE:source_tag} (?<annotations>[^{]*[^{ \n])(?<extra_fields> {.*})?" ]
  }

  # Use the log_timestamp as a date
  date {
    match => [ "log_timestamp", "ISO8601" ]
  }

  # Split and parse annotations:
  if [annotations] {
    mutate {
      split => { "annotations" => "," }
    }
    grok {
      match => { "annotations" => "(ip:%{IP:remote_ip})?" }
    }
    grok {
      match => { "annotations" => "(1: stream: %{GREEDYDATA:virus_name} FOUND)?" }
    }
  }

  # If there's a Remote IP, do a GeoIP lookup:
  if [remote_ip] {
    geoip {
      source => "remote_ip"
      target => "geoip" # Using this name as it's in the default index template
    }
  }

  # Parse the JSON chunk if present:
  if [extra_fields] {
    json {
      source => extra_fields
      target => "extra_fields"
    }
  }

  # Extract parts of the downloaded URI for analysis:
  if [downloaded_uri] =~ /^http.*/ {
    grok {
      match => [ "downloaded_uri", "%{WORD:downloaded_uri_scheme}://%{HOSTNAME:downloaded_uri_host}(?:%{NOTSPACE:downloaded_uri_path_and_query}|)" ]
    }
  } else if [downloaded_uri] != "-" {
    grok {
      match => [ "downloaded_uri", "%{WORD:downloaded_uri_scheme}:%{NOTSPACE:downloaded_uri_host}" ]
    }
  }

  # Extract parts of the fetch_timestamp if present:
  if [fetch_timestamp] != "-" {
    grok {
      match => [ "fetch_timestamp", "%{POSINT:fetch_start}\+%{NONNEGINT:fetch_duration:int}" ]
    }
  }
}

#
# Pass the resulting items to ElasticSearch in a dedicated index set:
#
output {
    elasticsearch {
      hosts => "elasticsearch:9200"
      manage_template => false
      index => "logstash-heritrix-%{[fields][crawl_id]}-%{+YYYY.MM.dd}"
    }
}
